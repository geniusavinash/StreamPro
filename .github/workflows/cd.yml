name: Continuous Deployment

on:
  push:
    branches: [ main ]
    tags: [ 'v*' ]
  workflow_run:
    workflows: ["Continuous Integration"]
    types:
      - completed
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event.workflow_run.conclusion == 'success'
    environment: staging
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'

    - name: Configure kubectl
      run: |
        aws eks update-kubeconfig --region ${{ secrets.AWS_REGION }} --name ${{ secrets.EKS_CLUSTER_NAME_STAGING }}

    - name: Deploy to staging
      run: |
        cd backend/k8s
        
        # Update image tags
        IMAGE_TAG="${{ github.sha }}"
        sed -i "s/newTag: .*/newTag: ${IMAGE_TAG}/" kustomization.yaml
        
        # Apply staging-specific configurations
        kubectl apply -k overlays/staging
        
        # Wait for rollout
        kubectl rollout status deployment/camera-streaming-backend -n camera-streaming-staging --timeout=600s
        
        # Verify deployment
        kubectl get pods -n camera-streaming-staging
        
        # Run health check
        kubectl wait --for=condition=ready pod -l app=camera-streaming-backend -n camera-streaming-staging --timeout=300s

    - name: Run smoke tests
      run: |
        # Wait for service to be ready
        sleep 30
        
        # Get service endpoint
        STAGING_URL=$(kubectl get ingress camera-streaming-ingress -n camera-streaming-staging -o jsonpath='{.spec.rules[0].host}')
        
        # Run basic health checks
        curl -f https://${STAGING_URL}/health || exit 1
        curl -f https://${STAGING_URL}/monitoring/health || exit 1
        
        echo "Smoke tests passed for staging deployment"

    - name: Notify deployment success
      uses: 8398a7/action-slack@v3
      with:
        status: success
        channel: '#deployments'
        text: 'ðŸš€ Successfully deployed to staging: https://${{ secrets.STAGING_DOMAIN }}'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [deploy-staging]
    if: startsWith(github.ref, 'refs/tags/v')
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'

    - name: Configure kubectl
      run: |
        aws eks update-kubeconfig --region ${{ secrets.AWS_REGION }} --name ${{ secrets.EKS_CLUSTER_NAME_PRODUCTION }}

    - name: Create backup
      run: |
        # Create database backup before deployment
        kubectl exec -n camera-streaming deployment/postgres -- pg_dump -U postgres camera_streaming > backup-$(date +%Y%m%d-%H%M%S).sql
        
        # Upload backup to S3
        aws s3 cp backup-*.sql s3://${{ secrets.BACKUP_BUCKET }}/database-backups/

    - name: Blue-Green Deployment
      run: |
        cd backend/k8s
        
        # Extract version from tag
        VERSION=${GITHUB_REF#refs/tags/v}
        IMAGE_TAG="${{ github.sha }}"
        
        # Update image tags
        sed -i "s/newTag: .*/newTag: ${IMAGE_TAG}/" kustomization.yaml
        
        # Deploy green environment
        kubectl apply -k overlays/production-green
        
        # Wait for green deployment to be ready
        kubectl rollout status deployment/camera-streaming-backend-green -n camera-streaming --timeout=600s
        
        # Run health checks on green environment
        kubectl wait --for=condition=ready pod -l app=camera-streaming-backend-green -n camera-streaming --timeout=300s
        
        # Switch traffic to green (blue-green switch)
        kubectl patch service camera-streaming-backend -n camera-streaming -p '{"spec":{"selector":{"version":"green"}}}'
        
        # Wait and verify
        sleep 30
        
        # Run production smoke tests
        PROD_URL=$(kubectl get ingress camera-streaming-ingress -n camera-streaming -o jsonpath='{.spec.rules[0].host}')
        curl -f https://${PROD_URL}/health || exit 1
        
        # If successful, scale down blue environment
        kubectl scale deployment camera-streaming-backend-blue --replicas=0 -n camera-streaming

    - name: Run production tests
      run: |
        # Run comprehensive production tests
        PROD_URL="https://${{ secrets.PRODUCTION_DOMAIN }}"
        
        # API health check
        curl -f ${PROD_URL}/health
        
        # Authentication test
        curl -f ${PROD_URL}/auth/health
        
        # Database connectivity test
        curl -f ${PROD_URL}/monitoring/health
        
        echo "Production tests passed"

    - name: Update monitoring
      run: |
        # Update Grafana dashboards
        kubectl apply -f backend/k8s/monitoring.yaml
        
        # Restart Prometheus to pick up new targets
        kubectl rollout restart deployment/prometheus -n monitoring

    - name: Notify deployment success
      uses: 8398a7/action-slack@v3
      with:
        status: success
        channel: '#deployments'
        text: 'ðŸŽ‰ Successfully deployed version ${{ github.ref_name }} to production: https://${{ secrets.PRODUCTION_DOMAIN }}'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  rollback:
    name: Rollback Production
    runs-on: ubuntu-latest
    if: failure() && needs.deploy-production.result == 'failure'
    needs: [deploy-production]
    environment: production
    
    steps:
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'

    - name: Configure kubectl
      run: |
        aws eks update-kubeconfig --region ${{ secrets.AWS_REGION }} --name ${{ secrets.EKS_CLUSTER_NAME_PRODUCTION }}

    - name: Rollback deployment
      run: |
        # Switch traffic back to blue environment
        kubectl patch service camera-streaming-backend -n camera-streaming -p '{"spec":{"selector":{"version":"blue"}}}'
        
        # Scale up blue environment
        kubectl scale deployment camera-streaming-backend-blue --replicas=3 -n camera-streaming
        
        # Wait for blue to be ready
        kubectl rollout status deployment/camera-streaming-backend-blue -n camera-streaming --timeout=300s
        
        # Scale down failed green environment
        kubectl scale deployment camera-streaming-backend-green --replicas=0 -n camera-streaming
        
        echo "Rollback completed successfully"

    - name: Restore database if needed
      run: |
        # Only restore if there were database changes
        if [ "${{ secrets.RESTORE_DATABASE }}" == "true" ]; then
          LATEST_BACKUP=$(aws s3 ls s3://${{ secrets.BACKUP_BUCKET }}/database-backups/ --recursive | sort | tail -n 1 | awk '{print $4}')
          aws s3 cp s3://${{ secrets.BACKUP_BUCKET }}/${LATEST_BACKUP} backup.sql
          kubectl exec -i -n camera-streaming deployment/postgres -- psql -U postgres camera_streaming < backup.sql
        fi

    - name: Notify rollback
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        channel: '#deployments'
        text: 'ðŸ”„ Production deployment failed and was rolled back. Please investigate.'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  cleanup:
    name: Cleanup
    runs-on: ubuntu-latest
    needs: [deploy-production]
    if: success()
    
    steps:
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Cleanup old images
      run: |
        # Keep only last 10 images
        aws ecr list-images --repository-name camera-streaming-backend --filter tagStatus=UNTAGGED --query 'imageIds[?imageDigest!=null]' --output json | \
        jq '.[:(-10)]' | \
        aws ecr batch-delete-image --repository-name camera-streaming-backend --image-ids file:///dev/stdin

    - name: Cleanup old backups
      run: |
        # Keep only last 30 days of backups
        aws s3 ls s3://${{ secrets.BACKUP_BUCKET }}/database-backups/ --recursive | \
        awk '$1 < "'$(date -d '30 days ago' '+%Y-%m-%d')'" {print $4}' | \
        xargs -I {} aws s3 rm s3://${{ secrets.BACKUP_BUCKET }}/{}

  security-scan-production:
    name: Production Security Scan
    runs-on: ubuntu-latest
    needs: [deploy-production]
    if: success()
    
    steps:
    - name: Run OWASP ZAP scan
      uses: zaproxy/action-full-scan@v0.7.0
      with:
        target: 'https://${{ secrets.PRODUCTION_DOMAIN }}'
        rules_file_name: '.zap/rules.tsv'
        cmd_options: '-a'

    - name: Upload ZAP scan results
      uses: actions/upload-artifact@v3
      with:
        name: zap-scan-results
        path: report_html.html

    - name: Notify security scan completion
      uses: 8398a7/action-slack@v3
      with:
        status: success
        channel: '#security'
        text: 'ðŸ”’ Security scan completed for production deployment'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}